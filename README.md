# Loading Data Lake with Spark through EMR Cluster

## Introduction
___

This project involves migrating Sparkify's user base and song database from Amazon S3 bucket, processing the data using Spark through Elastic MapReduce (EMR) cluster, and loading the data back to data lake (S3 bucket) as a set of dimensional tables. Sparkify is a music streaming app and their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

## Source Dataset
___
Source data sets are: <br>
- Song dataset: <br>
    The Song dataset contains metatdata about a song and its artist. Its files are stored in JSON format and are partitioned by the first three letters of each song's track ID.
    It resides in amazon S3 bucket, with the link below:

    <pre>s3://udacity-dend/song_data

    
- Log dataset: <br>
    The Log dataset contains log files in JSON format generated by an event simulator based on the songs dataset above. These simulate app activity logs from an imaginary music streaming app.
    It resided in amazon S3 bucket, with its link and json path specified respectively below:

    <pre>s3://udacity-dend/log_data

## Project Files
___

1. <code>etl.py</code>: Contains the python scripts to be executed, which ensures song and log source data are dimensioned and stored in the data lake (Amazon S3 bucket).

2. <code>dl.cfg</code>: is a configuration file that contains the filepaths to the song and log data sources, as well as path to the destination (data lake).

3. <code>README.md</code>: gives a summary of the project and an explanation of the files, and how to run them.

## Configuration Process
___
### EMR set up<br>
<pre>
1.  Set up credentials file (a .pem file):
    i. From the AWS console, click on Service, type 'EC2' to go to EC2  console.
    ii. Choose Key Pairs in Network & Security on the left panel => Choose Create key pair.
    iii. Type the name for the key pairs, such as "emr-cluster", File format: pem => Choose Create key pair.
    iv. After this step, a .pem file will be automatically downloaded.

2.  Create EMR cluster from the AWS console with the following components:
    release-label: emr-6.9.0 
    instance-count: 3 
    instance-type: m5.xlarge 
    instanceGroupType: MASTER
    instanceGroupType: CORE
    applications: JupyterHub, Spark, Hadoop

3. Wait for the created EMR cluster to display as status of "Waiting".

</pre>

### Allow SSH access
<pre>
    i. From the AWS console, click on Service, type EMR, and go to EMR console.
    ii. Choose Clusters => Choose the name of the cluster on the list which was created.
    iii. On the Summary tab, scroll down to see the part Security and access, choose the Security groups for Master link.
    iv. Choose the Security group ID for ElasticMapReduce-master.
    v. Scroll down on Inbound rules, 
    vi. Edit inbound rules => For safety, 
    vii. delete any SSH rule if have, then 
    viii. choose Add Rule => Choose Type: SSH, TCP for Protocol and 22 for Port Range => For source, select My IP
    ix. Choose Save
</pre>

### Connect to Primary Node using SSH
<pre>
The below process helps in operating EMR cluster from local terminal. Find process enumerated below: 
1. On the <b>Summary</b> tab under <b>Cluster management</b> section, locate the <b>Primary node public DNS<b> and select <b>Connect to the Primary Node using SSH</b>.
2. Select the particular Operating system you are using and copy the ssh command indicated. Example of ssh file
    <b>ssh -i ~/xxx.pem hadoop@ec2-xx-xxx-xxx.compute-1.amazonaws.com</b>
Ensure the location of the .pem file is properly indicated in the command
3. Paste the command in local terminal and press the Enter key.
Graphic bold letters spelling EMR is displayed on the screen indicating access to master node of EMR cluster through local terminal.
</pre>



## ETL Process
___
### Moving etl.py and dl.cfg files to EMR Cluster:<br>
<pre>
    1. After the access to the EMR cluster master node has been established, open another local terminal and run the command indicated below to move etl.py and dl.cfg files to the EMR cluster master node.

<b>scp -i ~/xxx.pem  ~/xxx/xxx/etl.py  hadoop@ec2-xx-xxx.compute-1.amazonaws.com:~/. </b>

<b>scp -i ~/xxx.pem  ~/xxx/xxx/dl.cfg  hadoop@ec2-xx-xxx.compute-1.amazonaws.com:~/. </b>

    2. Navigate to the directory of the etl.py file on the EMR cluster and run command spark-submit etl.py. 

    3. Once the ETL pipeline has finished executing, terminate EMR cluster and verify that parquet files were created in data lake.
</pre>
    







